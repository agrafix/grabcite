<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /vol2/faerberm/tools/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.5-dummy" ident="GROBID" when="2017-08-10T15:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Extraction of Systematic Polysemy Using Tree-cut</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Tomuro</surname></persName>
							<email>tomuro@cs.depaul.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">DePaul University School of Computer Science</orgName>
								<orgName type="department" key="dep2">Telecommunications and Information Systems</orgName>
								<address>
									<addrLine>243 S. Wabash Ave. Chicago</addrLine>
									<postCode>60604</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Extraction of Systematic Polysemy Using Tree-cut</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes an automatic method for extracting systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Systematic polysemy is a set of word senses that are related in systematic and predictable ways. Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. We compare the systematic relations extracted by our automatic method to manually extracted WordNet cousins.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>

			<note place="foot">However, those lexicons are rather complex. For instance, WordNet (version 1.6) contains a total of over 120,000 words and 170,000 word senses, which are grouped into around 100,000 synsets (synonym sets). In addition to the size, word entries in those lexicon are often polysemous. For instance, 20% of the words in Wordnet have more than one sense, and the average number of senses of those polysemous words is around 3. Also, the distinction between word senses tends to be ambiguous and arbitrary. For example, the following 6 senses are listed in WordNet for the noun &quot;door&quot;: 1. door-a swinging or sliding barrier 2. door-the space in a wall 3. door-anything providing a means of access (or escape) 4. door-a swinging or sliding barrier that will close off access into a car 5. door-a house that is entered via a door 6. door-a room that is entered via a door Because of the high degree of ambiguity, using such complex semantic lexicons brings some serious problems to the performance of NLP systems. The first, obvious problem is the computational intractability: increased processing time needed to disambiguate multiple possibilities will necessarily slow down the system. Another problem, which has been receiving attention in the past few years, is the inaccuracy: when there is more than one sense applicable in a given context, different systems (or human individuals) may select different senses as the correct sense. Indeed, recent studies in WSD show that, when sense definitions are fine-grained, similar senses become indistinguishable to human annotators and often cause disagreement on the correct tag (Ng et al., 1999; Veronis, 1998; Kilgarriff, 1998b). Also in IR and IE tasks, difference in the correct sense assignment will surely degrade recall and precision of the systems. Thus, it is apparent that, in order for a lexicon to be useful as an evaluation criteria for NLP systems, it must represent word senses at the level of granularity that captures human intuition. In Lexical Semantics, several approaches have been proposed which organize a lexicon based on systematic polysemy: 1 a set of word senses that are related in systematic and predictable ISystematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polyseray (Pustejovsky, 1995).</note>

			<note place="foot" n="2">Note that systematic polysemy should be contrasted with homonymy which refers to words which have more than one unrelated sense (e.g. FINANCIAL_INSTITUTION and SLOPING_LAND meanings of the word &quot;bank&quot;). 3Senses of &quot;dose&quot; in WordNet are: (1) a measured portion of medicine taken at any one time, and (2) the quantity of an active agent (substance or radiation) taken in or absorbed at any one time. ARTIFACT AIRCRAFT TOY /\ /l\ airplane helicopter ball kite puzzle Figure 1: An example thesaurus tree at this level, where differences between the systematic relations are rather clear, and therefore lexicons that encode word senses at this level of granularity have advantages over fine-grained as well as coarse-grained lexicons in various NLP tasks. Another issue we like to address is the ways for extracting systematic polysemy. Most often, this procedure is done manually. For example, the current version of WordNet (1.6) encodes the similarity between word senses (or synsets) by a relation called cousin. But those cousin relations were identified manually by the WordNet lexicographers. A similar effort was also made in the EuroWordnet project (Vossen et al., 1999). However, manually inspecting a large, complex lexicon is very time-consuming and often prone to inconsistencies. In this paper, we propose a method which automatically extracts systematic polysemy from a hierarchically organized semantic lexicon (WordNet). Our method uses a modification of a tree generalization technique used in (Li and Abe, 1998), and generates a tree-cut, which is a list of clusters that partition a tree. Then, we compare the systematic relations extracted by our automatic method to the WordNet cousins. Preliminary results show that our method discovered most of the WordNet cousins as well as some more interesting relations. 2 Tree Generalization using Tree-cut and MDL Before we present our method, we first give a brief summary of the tree-cut technique which we adopted from (Li and Abe, 1998). This technique is used to acquire generalized case frame patterns from a corpus using a thesaurus tree. 2.1 Tree-cut Models A thesaurus tree is a hierarchically organized lexicon where leaf nodes encode lexical data 21 (i.e., words) and internal nodes represent abstract semantic classes. A tree-cut is a partition of a thesaurus tree. It is a list of internal/leaf nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. Such set is also considered as a cluster. 4 Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint. For example, for a thesaurus tree in Figure 1, there are 5 tree-cuts: [airplane, helicopter, ball, kite, puzzle], [AIRCRAFT, ball, kite, puzzle], [airplane, helicopter, TOY], [AIRCRAFT, TOY] and [ARTIFACT]. Thus, a treecut corresponds to one of the levels of abstraction in the tree. Using a thesaurus tree and the idea of treecut, the problem of acquiring generalized case frame patters (for a fixed verb) from a corpus is to select the best tree-cut that accounts for both observed and unobserved case frame instances. In (Li and Abe, 1998), this generalization problem is viewed as a problem of selecting the best model for a tree-cut that estimates the true probability distribution, given a sample corpus data. Formally, a tree-cut model M is a pair consisting of a tree-cut F and a probability parameter vector O of the same length, M = (F, O) where F and ® are: (1) F=[Cx,..,Ck],O=[P(C,),..,P(Ck)] (2) words, that is, P(C) = ~=1 P(nj). Here, compared to knowing all P(nj) (where 1 &lt; j &lt; m) individually, knowing one P(C) can only facilitate an estimate of uniform probability distribution among members as the best guess, that is, P(nj) = P(C) for all j. Therefore, in general, m when clusters C1..Cm are merged and generalized to C according to the thesaurus tree, the estimation of a probability model becomes less accurate. 2.2 The MDL Principle To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL) principle (Rissanen, 1978). The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length). For the problem of case frame generalization, the MDL principle fits very well in that it captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution. The calculation of the description length for a tree-cut model is as follows. Given a thesaurus tree T and a sample S consisting of the case frame instances, the total description length L(M, S) for a tree-cut model M = (F, 0) is where Ci (1 &lt; i &lt; k) is a cluster in the treecut, P(Ci) is the probability of a cluster Ci, and ~/k=l P(Ci) = 1. For example, suppose a corpus contained 10 instances of verb-object relation for the verb &quot;fly&quot;, and the frequency of object noun n, denoted f(n), are as follows: f ( airpl ane )-5, f ( helicopter ) = 3, f ( bal l ) = O, f(kite)-2, f(puzzle) = 0. Then, the set of tree-cut models for the thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [0.5, 0.3, 0.2]) and ([AIRCRAFT, TOY], [0.8, 0.2]). Note that P(C) is the probability of cluster C = {nl, .., nm) as a whole. It is essentially the sum of all (true) probabilities of the member 4A leaf node is also a cluster whose cardinality is 1. L(M,S)=L(F)+L(eT)+L(SJF, e) (3) where L(F) is the model description length, L(OIF) is the parameter description length (explained shortly), and L(SIF , O) is the data description length. Note that L(F) + L(OIF ) essentially corresponds to the usual notion of the model description length. Each length in L(M, S) is calculated as follows. 5 The model description length L(F) is L(r) = log21GI (4) where G is the set of all cuts in T, and IG I denotes the size of G. This value is a constant for • SFor justification and detailed explanation of these formulas, see (Li and Abe, 1998).</note>

			<note place="foot">end, the notion of tree-cut and the MDL principle seem to comprise an excellent tool. However, we must be careful in adopting Li and Abe&apos;s technique directly: since the problem which their technique was applied to is fundamentally different from ours, some procedures used in their problem may not have any interpretation in our problem. Although both problems are essentially a tree generalization problem, their problem estimates the true probability distribution from a random sample of examples (a corpus), whereas our problem does not have any additional data to estimate, since all data (a lexicon) is already known. This difference raises the following issue. In the calculation of the data description length in equation (6), each word in a cluster, observed or unobserved, is assigned an estimated probability, which is a uniform fraction of the probability of the cluster. This procedure does not have interpretation if it is applied to our problem. Instead, we use the distribution of feature frequency proportion of the clusters, and calculate the data description length by the following formula: k L(SIF, e) =f(Ci) × log2P(Ci) (9) i=l where F = [C1,.., Ck], 0 = [P(C,),.., P(Ck)]. This corresponds to the length required to encode all words in a cluster, for all clusters in a tree-cut, assuming Huffman&apos;s algorithm (Huffman, 1952) assigned a codeword of length-log2P(Ci) to each cluster C/ (whose propor6We could also combine two (or possibly more) trees into one tree and apply clustering over that tree once. In this paper, we describe clustering of two trees for example purpose.</note>

			<note place="foot">M1 possible tree-cuts in a tree, a greedy dynamic programming algorithm is used. This algorithm , called Find-MDL in (Li and Abe, 1998), finds the best tree-cut for a tree by recursively finding the best tree-cuts for all of its subtrees and merging them from bottom up. This algorithm is quite efficient, since it is basically a depth-first search with minor overhead for computing the description length. Finally in the third step, clusters from the two tree-cuts are matched up, and the pairs which have substantial overlap are selected as systematic polysemy. Figure 4 shows parts of the final tree-cuts for ARTIFACT and MEASURE obtained by our method. ~ In both trees, most of the clusters in the tree-cuts are from nodes at depth 1 (counting the root as depth 0). That is because the tree-cut technique used in our method is sensitive to the structure of the tree. More specifically, the MDL principle inherently penalizes a complex tree-cut by assigning a long parameter length. Therefore, unless the entropy of the feature distribution is large enough to make the data length overshadow the parameter length, simpler tree-cuts partitioned at abstract levels are preferred. This situation tends to happen often when the tree is bushy and the total feature frequency is low. This was precisely the case with ARTIFACT and MEASURE, where both Tin the figure, bold letters indicate words which are polysemous in the two tree.</note>

			<note place="foot" n="4"> Evaluation To test our method, we chose 5 combinations from WordNet noun Top categories (which we call top relation classes), and extracted cluster pairs which have more than 3 overlapping words. Then we evaluated those pairs in two aspects: related vs. unrelated relations, and automatic vs. manual clusters. 4.1 Related vs. Unrelated Clusters Of the cluster pairs we extracted automatically, not all are systematically related; some are unrelated, homonymous relations. They are essentially false positives for our purposes. Table 1 shows the number of related and unrelated relations in the extracted cluster pairs. Although the results vary among category combinations, the ratio of the related pairs is rather low: less than 60% on average. There are several reasons for this. First, there are some pairs whose relations are spurious. For example, in ARTIFACT-GROUP class, a pair [LUMBER, SOCIAL_GROUP] was extracted. Words which are common in the two clusters are &quot;picket&quot;, &quot;board&quot; and &quot;stock&quot;. This relation is obviously homonymous. Second, some clusters obtained by tree-cut are rather abstract, so that pairing two abstract clusters results in an unrelated pair. For example, in ARTIFACT-MEASURE class, a pair [INSTRUMENTALITY, LINEAR_UNIT] was selected. Words which are common in the two clusters include &quot;yard&quot;, &quot;foot&quot; and &quot;knot&quot; (see the previous Figure 4). Here, the concept INSTRUMENTALITY is very general (at depth 1), and it also contains many (polysemous) words. So, matching this cluster with another abstract cluster is likely to yield a pair which has just enough overlapping words but whose relation is not systematic. In the case of [INSTRUMENTALITY, LINEAR_UNIT], the situation is even worse, because the concept of LINEAR_UNIT in MEASURE represents a collection of terms that were chosen arbitrarily in the his</note>

			<note place="foot" n="26"> a minor relation tends to be lost in our tree generalization procedure. However, the main reason is the difficulty mentioned earlier in the paper: the problem of applying the tree-cut technique to a bushy tree when the data is sparse. In addition to the WordNet cousins, our automatic extraction method discovered several interesting relations. Table 3 shows some examples,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, several on-line broad-coverage semantic lexicons became available, including LDOCE <ref type="bibr" target="#b15">(Procter, 1978)</ref>, WordNet <ref type="bibr" target="#b12">(Miller, 1990)</ref> and HECTOR. <ref type="bibr" target="#b9">(Kilgarriff, 1998a)</ref>. These lexicons have been used as a domainindependent semantic resource as well as an evaluation criteria in various Natural Language Processing (NLP) tasks, such as Information Retrieval (IR), Information Extraction (IE) and Word Sense <ref type="bibr">Disambiguation (WSD).1997</ref>) identified systematic relations that exist between abstract semantic concepts in the WordNet noun hierarchy, and defined a set of underspecified semantic classes that represent the relations. Then he extracted all polysemous nouns in WordNet according to those underspecified classes and built a lexicon called CORELEX. For example, a CORELEX class AQU (which represents a relation between ARTIFACT and QUANTITY) contains words such as "bottle", "bucket" and "spoon".</p><p>Using the abstract semantic classes and organizing a lexicon based on systematic polysemy addresses the two problems mentioned above in the following ways. For the first problem, using the abstract classes can reduce the size of the lexicon by combining several related senses into one sense; thus computation becomes more efficient. For the second problem, systematic polysemy does reflect our general intuitions on word meanings. Although the distinction between systematic vs. non-systematic relations (or related vs. unrelated meanings) is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example, <ref type="bibr">(Apresjan, 1973;</ref><ref type="bibr" target="#b7">Cruse, 1986;</ref><ref type="bibr" target="#b14">Nunberg, 1995;</ref><ref type="bibr" target="#b6">Copestake and Briscoe, 1995)</ref>).</p><p>However, there is one critical issue still to be addressed: the level of granularity at which the abstract classes are defined. The problem is that, when the granularity of the abstract classes is too coarse, systematic relations defined at that level may not hold uniformly at more fine-grained levels <ref type="bibr" target="#b19">(Vossen et al., 1999</ref>). For instance, the CORELEX class AQU mentioned above also contains a word "dose" .3 Here, the relation between the senses of "dose" is different from that of "bottle", "bucket" and "spoon", which can be labeled as CONTAINER-CONTAINERFUL relation. We argue that human intuitions can distinguish meanings</p><p>nES where, for each n E C and each C E F, and P(n)-P(C) ICl</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(C)-f(c) (8) ISl</head><p>Note here that, in (7), the probability of C is divided evenly among all n in C. This way, words that are not observed in the sample receive a non-zero probability, and the data sparseness problem is avoided.</p><p>Then, the best model is the one which requires the minimum total description length. <ref type="figure">Figure 2</ref> shows the MDL lengths for all five tree-cut models that can be produced for the thesaurus tree in <ref type="figure">Figure 1</ref>. The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle] indicated by a thick curve in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clustering Systematic Polysemy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalization Technique</head><p>Using the generalization technique in (Li and Abe, 1998) described in the previous section, we wish to extract systematic polysemy automatically from WordNet. Our assumption is that, if a semantic concept is systematically related to another concept, words that have one sense under one concept (sub)tree are likely to have another sense under the other concept (sub)tree. To give an example, <ref type="figure">Fig-ure 3</ref> shows parts of WordNet noun trees for ARTIFACT and MEASURE, where subtrees under CONTAINER and C0NTAINERFUL respectively contain "bottle", "bucket" and "spoon". Note a dashed line in the figure indicates an indirect link for more than one level.</p><p>Based on this assumption, it seems systematic polysemy in the two trees can be extracted straight-forwardly by clustering each tree according to polysemy as a feature, and by matching of clusters taken from each tree. 6 To thisA cousin relation in WordNet is defined between two synsets (currently in the noun trees only), and it indicates that senses of a word that appear in both of the (sub)trees rooted by those synsets are related, s The cousins were manuMly extracted by the WordNet lexicographers. Table 2 shows the number of cousins listed for each top relation class and the number of cousins our automatic method recovered (in the 'Auto' column). As you see, the total recall ratio is over 80% (27/33~ .82).</p><p>In the right three columns of <ref type="table">Table 2</ref>, we also show the breakdown of the recovered cousins, whether each recovered one was an exact match, or it was more general or specific than the corresponding WordNet cousin. From this, we can see that more than half of the recovered cousins were more general than the WordNet cousins. That is partly because some WordNet cousins have only one or two common words.</p><p>For example, a WordNet cousin <ref type="bibr">[PAINTING, COLORING_MATERIAL]</ref> in ARTIFACT-SUBSTANCE has only one common word "watercolor". Such</p><p>SActually, cousin is one of the three relations which indicate the grouping of related senses of a word. Others are sister and twin. In this paper, we use cousin to refer to all relations listed in "cousin.tps" file (available in a WordNet distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we proposed an automatic method for extracting systematic polysemy from WordNet. As we reported, preliminary results show that our method identified almost all WordNet cousins as well as some new ones. One difficulty is that applying the generalization technique using the MDL principle to the bushy WordNet trees seems to yield a tree-cut at rather abstract level. For future work, we plan to compare the systematic relations extracted by our automatic method to corpus data. In particular, we like to test whether our method extracts the same groups of senses which human annotators disagreed <ref type="bibr" target="#b13">(Ng et al., 1999</ref>). We also like to test whether our method agrees with the finding that multiple senses which occur in a discourse are often systematically polysemous <ref type="bibr" target="#b10">(Krovetz, 1998</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering Method</head><p>Our clustering method uses the the modified generalization technique described in the last section to generate tree-cuts. But before we apply the method, we must transform the data in Wordnet. This is because WordNet differs from a theaurus tree in two ways: it is a graph rather than a tree, and internal nodes as well as leaf nodes carry data, First, we eliminate multiple inheritance by separating shared subtrees. Second, we bring down every internal node to a leaf level by creating a new duplicate node and adding it as a child of the old node (thus making the old node an internal node).</p><p>After trees are transformed, our method extracts systematic polysemy by the following three steps. In the first step, all leaf nodes of the two trees are marked with either 1 or 0 (1 if a node/word appears in both trees, or 0 otherwise),</p><p>In the second step, the generalization technique is applied to each tree, and two tree-cuts are obtained. To search for the best tree-cut, instead of computing the description length for trees were quite bushy, and only 4% and 14% of the words were polysemous in the two categories respectively.  <ref type="table">Top relation class  WN cousin  ACTION-LOCATION  ARTIFACT-GROUP  ARTIFACT-MEASURE  ARTIFACT-SUBSTANCE  COMMUNICATION-PERSON  Total  33   Auto  Exact Gen Spec  2  1  0  1  0  6  6  1  5  0  1  1  0  1  0  15  13  3  9  1  9  6  5  1  0  27  9  17  1</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Action-L0cation Artifact-Group Artifact-Measure Artifact-Substance Communication-Person</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Total</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic vs Manual Clusters To compare the cluster pairs our method extracted automatically to manually extracted clusters, we use WordNet cousins. A cousin relation is relatively new in WordNet, and the coverage is still incomplete. However, it gives us a good measure to see whether our automatic method discovered systematic relations that correspond to human intuitions Regular Polysemy</title>
	</analytic>
	<monogr>
		<title level="j">References Apresjan, J. Linguistics</title>
		<imprint>
			<biblScope unit="issue">142</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Lexicon for Underspecified Semantic Tagging In Proceedings off the A CL SIGLEX Workshop on Tagging Text Table 3: Examples of Automatically Extracted Systematic Polysemy Top relation class ACTION-LOCATION Relation [ACTION, POINT] Common Words [VOICE, SINGER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ARTIFACT-GROUPSTRUCTURE</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artifact-Substance [ Fabric</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chemical_Compound ] Communi Cati 0n-Person [</forename><surname>Writing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Religious-Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jeremiah&quot; with Lexical Semantics</title>
		<imprint>
			<publisher>Matthew</publisher>
			<biblScope unit="page" from="25" to="33" />
			<publisher>Matthew</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CORELEX: Systematic Polysemy and Underspecification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buitelaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semiproductive Polysemy and Sense Extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lexical Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cruse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Model for the Construction of Minimum Redundancy Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SENSEVAL: An Exercise in Evaluating Word Sense Disambiguation Programs Inter-tagger Agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC Kilgarriff, Advanced Papers of the SENSEVAL Workshop</title>
		<meeting>the LREC Kilgarriff, Advanced Papers of the SENSEVAL Workshop<address><addrLine>Sussex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">More than One Sense Per Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Papers of the SENSEVAL Workshop</title>
		<meeting><address><addrLine>Sussex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalizing Case Frames Using a Thesaurus and the MDL Principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="244" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">WORDNET: An Online Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Case Study on Inter-Annotator Agreement for Word Sense Disambiguationl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the A CL SIGLEX Workshop on Standardizing Lexical Resources</title>
		<meeting>the A CL SIGLEX Workshop on Standardizing Lexical Resources<address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfers of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nunberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Longman dictionary of Contemporary English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Procter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<pubPlace>Longman Group</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Generative Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Modeling by Shortest Data Description. Automatic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Study of Polysemy Judgements and Inter-annotator Agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Papers of the SENSEVAL Workshop</title>
		<meeting><address><addrLine>Sussex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards a Universal Index of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the A CL SIGLEX Workshop on Standardizing Lexical Resources</title>
		<meeting>the A CL SIGLEX Workshop on Standardizing Lexical Resources<address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
