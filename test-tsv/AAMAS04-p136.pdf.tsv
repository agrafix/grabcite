feature	text	page	minX	minY	maxX	maxY	mostCommonFont	mostCommonFontsize	mostCommonColor	startFont	startFontsize	startColor	endFont	endFontsize	endColor	role
paragraph	Approximate Solutions For Partially Observable Stochastic Games with	1	86.52	675.42	525.51373	683.69055	font-235	14.3	color-0	font-235	14.3462	color-0	font-235	14.3462	color-0	title
paragraph	Common Payoffs	1	253.43948	657.7799	358.5598	666.0505	font-235	14.3	color-0	font-235	14.3462	color-0	font-235	14.3462	color-0	section-heading
paragraph	RosemaryE{rmemeeryrSCy-cM,aghrPgoonooinetrlgttdseoibomefunCMer,rsgoelchomlh,l,onpPGneuAietdUeo1erfn5f}Si2@vGc1eio3cerssrndi.ctcoyemnu,.JeedffuSchneider tSShttSrSaauetnnanbffno@oafrrsoddstir,tadaUCnnAnAfTioIvh9rLedr4rau.3sebni0dty5u	1	96.12	562.50977	515.8181	628.3854	font-238	12.0	color-0	font-237	11.9552	color-0	font-238	11.9552	color-0	formula
paragraph	Abstract	1	154.43988	527.57776	198.92517	534.4699	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	abstract-heading
paragraph	Partially observable decentralized decision making in robot teams is fundamentally different from decision making in fully observable problems. Team members cannot simply apply single-agent solution techniques in parallel. Instead, we must turn to game theoretic frameworks to correctly model the problem. While partially observable stochastic games (POSGs) provide a solution model for decentralized robot teams, this model quickly becomes intractable. We propose an algorithm that approximates POSGs as a series of smaller, related Bayesian games, using heuristics such as QMDP to provide the future discounted value of actions. This algorithm trades off limited look-ahead in uncertainty for computational feasibility, and results in policies that are locally optimal with respect to the selected heuristic. Empirical results are provided for both a simple problem for which the full POSG can also be constructed, as well as more complex, robot-inspired, problems.	1	58.5596	320.69998	294.40323	513.61945	font-238	10.0	color-0	font-238	9.9626	color-0	font-238	9.9626	color-0	unknown
paragraph	1. Introduction	1	58.56	289.38	140.66234	296.2722	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	Partially observable problems, those in which agents do not have full access to the world state at every timestep, are very common in robotics applications where robots have limited and noisy sensors. While partially observable Markov decision processes (POMDPs) have been success-fully applied to single robot problems [11], this framework does not generalize well to teams of decentralized robots. All of the robots would have to maintain parallel POMDPs representing the teams’ joint belief state, taking all other robot observations as input. Not only does this algorithm scale exponentially with the number of robots and observations, it requires unlimited bandwidth and instantaneous communication between the robots. If no communication is available to the team, this approach is no longer correct: each robot will acquire different observations, leading to different belief states that cannot be reconciled. Therefore, we conjecture that robots with limited knowledge of their Permission to make digital or hard copies of all or part of  	1	18.408	72.0417	295.1076	275.18445	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	1	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	1	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	1	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	1	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	1	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	1	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	1	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	1	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	MDP	1	331.1036	522.526	348.63947	528.19293	font-218	7.9	color-0	font-218	7.8927	color-0	font-218	7.8927	color-0	unknown
paragraph	POMDP POSG	1	419.6498	520.9832	542.78894	528.3429	font-218	7.9	color-0	font-218	7.8927	color-0	font-218	7.8927	color-0	unknown
paragraph	state space state space belief space belief space	1	324.0681	466.30786	547.991	471.72833	font-218	5.8	color-0	font-218	5.8035	color-0	font-218	5.8035	color-0	unknown
paragraph	Figure 1. A representation of the relationship	1	329.16	448.0181	541.524	457.4228	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	figure-caption
paragraph	between MDPs, POMDPs and POSGs.	1	329.16	436.74643	507.46927	445.77252	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	unknown
paragraph	teammates’ observations must reason about all possible belief states that could be held by teammates and how that affects their own action selection. This reasoning is tied to the similarity in the relationship between partially observable stochastic games (POSGs) and POMDPs to the relationship between POMDPs and Markov decision processes (MDPs) (see Figure 1). Much in the same way that a POMDP cannot be solved by randomizing over solutions to different MDPs, it is not sufficient to find a policy for a POSG by having each robot solve parallel POMDPs with reasonable assumptions about teammates’ experiences. Additional structure is necessary to find solutions that correctly take into account different experiences.	1	317.28	269.5781	553.7742	419.18445	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	We demonstrate this with a simple example. Consider a team of paramedic robots in a large city. When a distress signal, e, is received by emergency services, a radio call, rc, is sent to the two members of the paramedic team. Each robot independently receives that call with probability 0.9 and must decide whether or not to respond to the call. If both robots respond to the emergency then it is dealt with (cost of 0), but if only one robot responds then the incident is unresolved with a known cost c that is either 1.0 or 100.0 with equal probability (low or high cost emergencies). Alternatively, a robot that receives a radio call may relay that a call to its teammate at a cost d of 1.0 and both robots then respond to the emergency. What should a robot do when it receives a radio call?	1	317.28	105.88058	553.6546	265.22443	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	If the two robots run POMDPs in parallel in order to decide their policies then they have to make assumptions about their teammates’ observation and policy in order to make a decision. First, robot 1 can assume that robot 2 always receives and responds to the radio call when robot 1 receives that call. Robot 1 should therefore also always respond to the call, resulting in a cost of 5.05 (0.9c). Alternatively, robot 1 can assume robot 2 receives no information and so it relays the radio call because otherwise it believes that robot 2 will never respond to the emergency. This policy has a cost of 1.0. A third possibility is for each robot to calculate the policy for both of these POMDPs and then randomize over them in proportion to the likelihood that each POMDP is valid. This, however, leads to a cost of 4.654. The robots can do better by considering the distribution over their teammate’s belief state directly in policy construction. A policy of relaying a radio call when d < 0.9c and otherwise just responding to the call has a cost of 0.55 which is better than any of the POMDP variants. The POSG is a game theoretic approach to multi-agent decision making that implicitly models a distribution over other agents’ belief states. It is, however, intractable to solve large POSGs, and we propose that a viable alternative is for agents to interleave planning and execution by building and solving a smaller approximate game at every timestep to generate actions. Our algorithm will use information common to the team (e.g. problem dynamics) to approximate the policy of a POSG as a concatenation of a series of policies for smaller, related Bayesian games. In turn, each of these Bayesian games will use heuristic functions to evaluate the utility of future states in order to keep policy computation tractable. This transformation is similar to the classic one-step lookahead strategy for fully-observable games [13]: the agents perform full, game-theoretic reasoning about their current knowledge and first action choice but then use a predefined heuristic function to evaluate the quality of the resulting outcomes. The resulting policies are always coordinated; and, so long as the heuristic function is fairly accurate, they perform well in practice. A similar approach, proposed by Shi and Littman [14], is used to find near-optimal solutions for a scaled down version of Texas Hold’Em.	1	58.55999	78.76836	553.6945	716.7844	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	2. Partially Observable Stochastic Games	2	58.56	229.74	274.27124	236.63217	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	Stochastic games, a generalization of both repeated games and MDPs, provide a framework for decentralized action selection [4]. POSGs are the extension of this framework to handle uncertainty in world state. A POSG is defined as a tuple (I, S, A, Z, T, R, O). I = {1, ..., n} is the set of agents, S is the set of states and A and Z are respectively the cross-product of the action and observation space for each agent, i.e. A = A1 × ··· × An. T is the transition function, T : S × A → S, R is the reward function, R : S × A →  and O defines the observation emission probabilities, O : S × A × Z → [0, 1]. At each timestep of a POSG the agents simultaneously choose ac-Permission to make digital or hard copies of all or part of   	2	18.408	72.0417	295.06076	216.62447	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	2	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	2	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	2	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	2	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	2	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	2	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	2	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	2	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	tions and receive a reward and observation. In this paper, we limit ourselves to finite POSGs with common payoffs (each agent has an identical reward function R).	2	317.27838	684.40564	553.8435	716.7821	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Agents choose their actions by solving the POSG to find a policy σi for each agent that defines a probability distribution over the actions it should take at each timestep. We will use the Pareto-optimal Nash equilibrium as our solution concept for POSGs with common payoffs. A Nash equilibrium of a game is a set of strategies (or policies) for each agent in the game such that no one agent can improve its performance by unilaterally changing its own strategy [4]. In a common payoff game, there will exist a Pareto-optimal Nash equilibrium which is a set of best response strategies that maximize the payoffs to all agents. Coordination protocols [2] can be used to resolve ambiguity if multiple Pareto-optimal Nash equilibria exist.	2	317.2784	531.4176	553.8309	680.9018	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	The paramedic problem presented in Section 1 can be represented as a POSG with: I = {1, 2}, S = {e, e}; Ai = {respond,relay-call,do-nothing}; and Zi = {rc, rc}. The reward function is defined as a cost c if both robots do not respond to an emergency and a cost d of relaying a radio call. T and O can only be defined if the probability of an emergency is given. If they were given, each robot could then find a Pareto-optimal Nash equilibrium of the game.	2	317.27985	436.84845	554.02686	527.90375	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	There are several ways to represent a POSG in order to find a Nash equilibrium. One useful way is as an extensive form game [4] which is an augmented game tree. Extensive form games can then be converted to the normal form [4] (exponential in the size of the game tree) or the sequence form [15, 6] (linear in the size of the game tree) and, in theory, solved. While this is non-trivial in arbitrary n-player games, to solve games with common payoff problems, we can use an alternating-maximization algorithm to find locally optimal best response strategies. Holding the strategies of all but one agent fixed, linear or dynamic programming is used to find a best response strategy for the remaining agent. We optimize for each agent in turn until no agent wishes to change its strategy. The resulting equilibrium point is a local optimum; so, we use random restarts to explore the strategy space.	2	317.28168	248.68884	553.87537	433.46515	font-237	10.0	color-0	font-237	1.0	color-0	font-237	9.9626	color-0	body-text
paragraph	Extensive form game trees provide a theoretically sound framework for representing and solving POSGs; however, because the action and observation space are exponential in the number of agents, for even the smallest problems these trees rapidly become too large to solve. Instead, we turn to more tractable representations that trade off global optimal-ity for locally optimal solutions that can be found efficiently.	2	317.28168	166.0092	553.86896	245.30554	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	3. Bayesian Games	2	317.28	137.34	418.43896	144.23216	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	We will be approximating POSGs as a series of Bayesian games. As it will be useful to have an understanding of Bayesian games before describing our transformation, we first present an overview of this game theory model. Bayesian games model single state problems in which each agent has private information about something relevant to the decision making process [4]. While this private information can range from knowledge about the number of agents in the game to the action set of other agents, in general it can be represented as uncertainty about the utility (or payoffs) of the game. In other words, utility depends on this private information. For example, in the paramedic problem, the private information known by robot 1 is whether or not it has heard a radio call. If both robots know whether or not each other received the call, then the payoff of the game is known with certainty and action selection is straightforward. But as this is not the case, each robot has uncertainty over the payoff. In the game theory literature, the private information held by an agent is called its type, and it encapsulates all non-commonly-known information to which the agent has access. The set of possible types for an agent can be infinite, but we limit ourselves to games with finite sets. Each agent knows its own type with certainty but not those of other agents; however, beliefs about the types of others are given by a commonly held probability distribution overjoint types. If I = {1, ..., n} is the set of agents, θi is a type of agent i and Θi is the type space of agent i such that θi ∈ Θi, then a type profile θ is θ = {θi, ..., θn} and the type profile space is Θ = Θ1×· · ·×Θn. θ−i is used to represent the type of all agents but i, with Θ−i defined similarly. A probability distribution, p ∈ Δ(Θ), over the type profile space Θ is used to assign types to agents, and this probability distribution is assumed to be commonly known. From this probability distribution, the marginal distribution over each agent’s type, pi ∈ Δ(Θi), can be recovered as can the conditional proba-In	2	58.242973	78.76814	554.05316	716.7844	font-237	10.0	color-0	font-21	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	bilities pi(θ−i|θi).	3	58.55997	316.76935	131.61023	326.73196	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	formula
paragraph	the paramedic example Θi = {rc, rc}. The conditional probability pi(rcj|rci) = 0.9 and pi(rcj|rci) = 0.1. The description of the problem does not give enough information to assign a probability distribution over Θ= {{rc, rc}, {rc, rc}, {rc, rc}, {rc, rc}}. The utility, u, of an action ai to an agent is dependent on the actions selected by all agents as well as on their type profile and is defined as ui(ai, a−i, (θi, θ−i)). By definition, an agent’s strategy must assign an action for every one of its possible types even though it will only be assigned one type. For example, although robot 1 in the paramedic example either receives a call, rc, or does not, its strategy must still define what it would do in either case. Let an agent’s strategy be σi, and the probability distribution over actions it assigns for θi be given by p(ai|θi) = σi(θi). Formally, a Bayesian game is a tuple (I, Θ, A, p, u) where A = A1 × · · · × An, u = {ui, ..., un} and I, Θ and p are as defined above. Given the commonly held prior p(Θ) over the distribution over agents’ types, each agent can calculate a Bayesian-Nash equilib-Permission to make digital or hard copies of all or part of   	3	18.408	72.0417	295.07297	313.77194	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	3	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	3	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	3	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	3	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	3	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	3	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	3	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	3	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	rium policy. This is a set of best response strategies σ in which each agent maximizes its expected utility conditioned on its probability distribution over the other agents’ types. In this equilibria, each agent has a policy σi that, given σ−i, maximizes the expected value u(σi, θi, σ−i) = ∑θ−i∈Θ−i pi(θ−i|θi)ui(σi(θi), σ−i(θ−i), (θi, θ−i)). Agents know σ−i because an equilibrium solution is defined as σ, not just σi.	3	317.27808	625.8478	553.6223	716.782	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Bayesian-Nash equilibria are found using similar techniques as for Nash equilbria in an extensive form game. For a common-payoff Bayesian game where ui = uj∀i, j, we can find a solution by applying the sequence form and the alternating maximization algorithm as described in Section 2.	3	317.2803	557.4706	553.70074	622.9441	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	4. Bayesian Game Approximation	3	317.28	534.66	494.68567	541.5521	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	We propose an algorithm for finding an approximate solution to a POSG with common payoffs that transforms the original problem into a sequence of smaller Bayesian games that are computationally tractable. So long as each agent is able to build and solve the same sequence of games, the team can coordinate on action selection. Theoretically, our algorithm will allow us to handle finite horizon problems of indefinite length by interleaving planning and execution. While we will formally describe our approximation in Algorithms 1 and 2, we first present an informal overview of the approach.	3	317.2796	395.33835	554.0171	521.54456	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	In Figure 2, we show a high-level diagram of our algorithm. We approximate the entire POSG, shown by the outer triangle, by constructing a smaller game at each timestep. Each game models a subset of the possible experiences occurring up to that point and then finds a one-step lookahead policy for each agent contingent on that subset.	3	317.2796	325.0084	553.7506	392.54483	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	The question is now how to convert the POSG into these smaller slices. First, in order to model a single time slice, it is necessary to be able to represent each sub-path through the tree up to that timestep as a single entity. A single sub-path through the tree corresponds to a specific set of observation and action histories up to time t for all agents in the team. If all agents know that a specific path has occurred, then the problem becomes fully observable and the payoffs of taking each joint action at time t are known with certainty. This is analogous to the utility in Bayesian games being conditioned on specific type profiles, and so we model each smaller game as a Bayesian game. Each path in the POSG up to time t is now represented as a specific type profile, θt, with the the type of each agent, corresponding to its own observation and actions, as θti. A single θti may appear in multiple θt. For example, in the paramedic problem, robot 1’s type θ1 = rc appears in both θ = {rc, rc} and θ = {rc, rc}.	3	317.27954	113.72905	553.99493	322.10428	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Agents can now condition their policies on their individual observation and action histories; however, there are still two pieces of the Bayesian game model left to define before	3	317.2795	78.76759	553.7656	111.14406	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Full POSG OLonoek-Sahteepad Game at time t	4	127.7513	686.0486	221.99423	706.8255	font-218	5.3	color-0	font-218	5.2796	color-0	font-218	5.2796	color-0	formula
paragraph	Figure 2. A high level representation of our algorithm for approximating POSGs.	4	70.44	605.4583	282.8809	626.6228	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	figure-caption
paragraph	tσI=n00=it,siahol1liv=zeA{e}G,gΘae0m,pne(t(ΘΘ10)0,p(Θ0)) tσI=n00=it,siahol2liv=zAe{e}G,gΘae0m,pne((ΘtΘ02)0,p(Θ0)) tσI=n00=it,siahoAlnliv=zeg{e}G,eΘan0m,pte((ΘnΘ0)0,p(Θ0))	4	75.19465	552.373	291.53738	579.9131	font-222	5.2	color-0	font-222	5.1737	color-0	font-222	5.1737	color-0	formula
paragraph	Mha1k=eoobbss1teUrvha1tion Mha2k=eoobbss2teUrvha2tion Mhank=eoobbssnteUrvhantion	4	76.1079	525.1407	279.89832	538.9348	font-218	5.2	color-0	font-218	5.1737	color-0	font-218	5.1737	color-0	formula
paragraph	Mθa1tt=chbehsitsMtoartcyht(oht1,yΘp1te) Mθa2tt=chbehsitsMtoartcyht(oht2,yΘp2te) Mθantt=chbehsitsMtoartcyht(ohtn,yΘpnte)	4	75.9557	495.76663	288.07755	509.56656	font-222	5.2	color-0	font-218	5.1737	color-0	font-222	5.1737	color-0	formula
paragraph	Eax1te=cσu1t(tθe1t)action Eax2te=cσu2t(tθe2t)action Eaxnte=cσunt(tθent)action	4	76.1079	466.55054	272.7109	480.35034	font-218	5.2	color-0	font-218	5.1737	color-0	font-218	5.1737	color-0	formula
paragraph	PΘrot+1p,pa(gΘatt+e1)forward PΘrot+1p,pa(gΘatt+e1)forward PΘrot+1p,pa(gΘatt+e1)forward	4	75.9557	437.33435	281.75412	451.1445	font-218	5.2	color-0	font-218	5.1737	color-0	font-218	5.1737	color-0	formula
paragraph	σtFt=+i1n=td+s1oplvoeliGcyamfoer(Θt+t,1p(Θt)) σtFt=+i1n=td+s1oplvoeliGcyamfoer(Θt+t,1p(Θt)) σtFt=+i1n=td+s1oplvoeliGcyamfoer(Θt+t,1p(Θt))	4	74.58641	407.0224	291.86047	421.77618	font-222	5.2	color-0	font-234	5.1737	color-0	font-222	5.1737	color-0	formula
paragraph	Figure 3. A diagram showing the parallel nature of our algorithm. Shaded boxes indicate steps that have the same outcome for all agents.	4	70.44	351.5381	282.88968	396.0829	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	figure-caption
paragraph	these policies can be found. First, the Bayesian game model requires agents to have a common prior over the type profile space Θ. If we assume that all agents have common knowledge of the starting conditions of the original POSG, i.e. a probability distribution over possible starting states, then our algorithm can iteratively find Θt+1 and p(Θt+1) using information from Θt, p(Θt), A, T, Z, and O. Additionally, because the solution of a game, σ, is a set of policies for all agents, this means that each agent not only knows its own next-step policy but also those of its teammates and this information can be used to update the type profile space. As the set of all possible histories at time t can be quite large, we discard all types with prior probability less than some threshold and renormalize p(Θ). Second, it is necessary to define a utility function u that represents the payoff of actions conditioned on the type profile space. Ideally, a utility function u should represent not only the immediate value of a joint action but also its expected future value. In finite horizon MDPs and POMDPs, these future values are found by backing up the value of actions at the final timestep back through time. In our algorithm, this would correspond to finding the solution to Permission to make digital or hard copies of all or part of  	4	18.408	72.0417	295.3033	335.06454	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	4	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	4	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	4	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	4	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	4	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	4	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	4	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	4	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	the Bayesian game representing the final timestep T of the problem, using that as the future value of the game at the time T − 1, solving that Bayesian game and then backing up that value through the tree. Unfortunately, this is intractable; it requires us to do as much work as solving the original POSG because we do not know a specific probability distribution over ΘT until we have solved the game for timesteps 0 through T − 1 and we cannot take advantage of any pruning of Θt. Instead, we will perform our one-step lookahead using a heuristic function for u, resulting in policies that are locally optimal with respect to that heuristic. We will use heuristic functions that try to capture some notion of future value while remaining tractable to calculate.	4	317.27768	567.1684	553.8161	716.7815	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Now that the Bayesian game is fully defined, the agents can iteratively build and solve these games for each timestep. The t-th Bayesian game is converted into its corresponding sequence form and a locally optimal Bayes-Nash equilibrium found by using the alternating-maximization algorithm. Once a set of best-response strategies is found at time t, each agent i then matches its true history of observations and actions, hi, to one of the types in its type space Θi and then selects an action based on its policy, σti, and type, θti.1	4	317.28003	449.51022	553.70557	564.3849	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	This Bayesian game approximation runs in parallel on all agents in the team as shown in Figure 3. So long as a mechanism exists to ensure that each agent finds the same set of best-response policies σt, then each agent will maintain the same type profile space for the team as well as the same probability distribution over that type space without having to communicate. This ensures that the common prior assumption is always met. Shading is used in Figure 3 to indicate which steps of the algorithm should result in the same output for each agent. Agents run the algorithm in lock-step and a synchronized random-number generator is used to ensure all agents come up with the same set of best-response policies using the alternating-maximization algorithm. Only common knowledge arising from these policies or domain specific information (e.g. agents always know the position of their teammates) is used to update individual and joint type spaces. This includes pruning of types due to low probability as pruning thresholds are common to the team. The non-shaded boxes in Figure 3 indicate steps in which agents only use their own local information to make a decision. These steps involve matching an agent’s true observation history, hi to a type modeled in the game, θti, and then executing an action based on that type.	4	317.2796	180.4083	554.001	447.14453	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	In practice there are two possible difficulties with constructing and solving our Bayesian game approximation of the POSG. The first is that the type space Θ can be extremely large, and the second is that it can be difficult to	4	317.28073	133.48842	554.04083	177.62474	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	1 If the agent’s true history was pruned, we map the actual history to	4	317.28	109.20252	553.4771	116.38358	font-237	8.0	color-0	font-237	7.9701	color-0	font-237	7.9701	color-0	unknown
paragraph	the nearest non-pruned history using Hamming distance as our metric. This minimization should be done in a more appropriate space such as the belief space, and is future work.	4	331.43967	82.81033	553.36774	107.62365	font-237	8.0	color-0	font-237	7.9701	color-0	font-237	7.9701	color-0	unknown
paragraph	Algorith  1: PolicyConstructionAndExecution	5	58.56	708.1776	268.998	717.3532	font-21	10.0	color-0	font-235	9.9626	color-0	font-21	9.9626	color-0	unknown
paragraph	I, Θ0, A, p(Θ0), Z, S, T, R, O	5	70.44	695.14746	172.98021	703.9951	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	Output: r, st, σt, ∀t	5	70.44013	686.8418	137.821	694.87604	font-26	8.0	color-0	font-235	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	begin	5	70.440186	677.7001	89.0424	682.29486	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	hi ←− ∅, ∀i ∈I	5	85.8	667.4019	142.17805	675.0851	font-62	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	r ←− 0	5	85.799164	659.89246	112.4719	665.3599	font-62	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	initializeState s0	5	85.79837	651.1325	151.73232	658.1552	font-26	8.0	color-0	font-26	7.9701	color-0	font-35	5.9776	color-0	unknown
paragraph	for t ← 0 to t(max d)o	5	85.80024	641.52026	157.09642	657.2776	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	section-heading
paragraph	for i ∈ I do (in parallel)	5	101.16	633.26996	183.27672	639.22363	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	formula
paragraph	setRandSeed rst	5	116.52	624.73267	179.73132	631.51605	font-26	8.0	color-0	font-26	7.9701	color-0	font-24	5.9776	color-0	unknown
paragraph	σt, Θt+1, p(Θ(t+1))←	5	116.51985	614.14746	192.55266	630.8776	font-37	8.0	color-0	font-26	7.9701	color-0	font-62	7.9701	color-0	formula
paragraph	BayesianGame(I, Θt, A, p(Θt), Z, S, T, R, O, rst) hi ← hi ∪ zti ∪ at−1i	5	116.52008	594.12006	297.89166	613.87604	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	θti ← matchToType(hi, Θti)	5	116.52	583.92053	218.09166	593.236	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	ati ← σti(θti)	5	116.5199	573.60034	160.13165	583.036	font-24	6.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	st+1 ← T(st, at1, ..., atn)	5	101.16	559.3797	187.37166	568.8751	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	r ← r + R(st, at1, ..., atn)	5	101.15988	549.29987	189.65166	558.43604	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	end	5	70.44	535.38	82.84148	539.9748	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	section-heading
paragraph	find a good heuristic for u.	5	58.56	514.4881	164.97151	523.4644	font-237	10.0	color-0	font-237	1.0	color-0	font-237	9.9626	color-0	body-text
paragraph	We can exert some control over the size of the type space by choosing the probability threshold for pruning unlikely histories. The benefit of using such a large type space is that we can guarantee that each agent has sufficient information to independently construct the same set of histories Θt and the same probability distribution over Θt at each timestep t. In the future, we plan on investigating ways of sparsely populating the type profile space or possibly recreating comparable type profile spaces and probability distributions from each agent’s own belief space. Providing a good u that allows the Bayesian game to find high-quality actions is more difficult because it depends on the system designer’s domain knowledge. We have had good results in some domains with a QMDP heuristic [7]. For this heuristic, u(a, θ) is the value of the joint action a when executed in the belief state defined by θ assuming that the problem becomes fully observable after one timestep. This is equivalent to the QMDP value of that action in that belief state. To calculate the QMDP values it is necessary to find Q-values for a fully observable version of the problem. This can be done by dynamic programming or Q-learning with either exact or approximate Q-values depending upon the nature and size of the problem. The algorithms for the Bayesian game approximation are shown in Algorithm 1 and 2. Algorithm 1 takes the parameters of the original POSG (the initial type profile space is generated using the initial distribution over S) and builds up a series of one-step policies, σt, as shown in Figure 3. In parallel, each agent will: solve the Bayesian game for timestep t to find σt; match its current history of experiences to a specific type θti in its type profile; match this type to an action as given by its policy σti; and then execute that action. Algorithm 2 provides an implementation of how to solve a Bayesian game given the dynamics of the overall POSG and a specific type profile space and distribution over that space. It also propagates the type profile space and its prior probability forward one timestep based on the Permission to make digital or hard copies of all or part of  	5	18.408	72.0417	295.09866	510.98425	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	5	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	5	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	5	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	5	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	5	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	5	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	5	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	5	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	Algorith  2: BayesianGame	5	317.28	708.29767	443.7551	717.36365	font-21	10.0	color-0	font-235	9.9626	color-0	font-21	9.9626	color-0	unknown
paragraph	Input: I, Θ, A, p(Θ), Z, S, T, R, O, randSeed	5	329.16	695.86743	486.60538	703.8376	font-26	8.0	color-0	font-235	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	Output: σ, Θ′, p(Θ′)	5	329.16003	684.2274	399.77167	692.3215	font-235	8.0	color-0	font-235	7.9701	color-0	font-37	7.9701	color-0	unknown
paragraph	begin	5	329.15976	675.06024	347.76196	679.655	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	setSeed randSeed	5	344.52	666.1326	412.47885	671.91095	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	for a ∈ A( θ ∈ Θ do)	5	344.51996	657.02997	415.7352	672.2776	font-235	8.0	color-0	font-235	7.9701	color-0	font-37	7.9701	color-0	section-heading
paragraph	u(a, θ,) ← qmdpValue(a, beliefState(θ))	5	359.88	646.7875	507.41534	658.42474	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	σ ← findPolicies(I, Θ, A, p(Θ), u)	5	344.52	634.4274	471.89734	642.3976	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	Θ′ ←− ∅	5	344.5192	626.9263	376.35385	633.7615	font-62	8.0	color-0	font-37	7.9701	color-0	font-62	7.9701	color-0	formula
paragraph	Θfori′ θ←∈−Θ∅,,z∀i∈∈ZI, a ∈ A do	5	344.51968	609.51	436.1011	625.00146	font-62	8.0	color-0	font-37	7.9701	color-0	font-235	7.9701	color-0	formula
paragraph	φ←θ∪z∪a	5	359.88	599.6341	409.8565	606.871	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	p(φ) ← p(z, a|θ)p(θ)	5	359.6879	590.5075	433.85367	598.47766	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	if p(φ) > pruningThreshold then	5	359.8784	581.6272	468.45343	589.59735	font-238	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	θ′ ←φ	5	375.24	573.2341	400.3174	581.0816	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	p(θ′) ← p(φ)	5	375.04822	564.1074	421.9731	572.2015	font-26	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	Θ′ ← Θ′ ∪ θ′	5	375.24042	557.17267	422.7878	563.44147	font-60	8.0	color-0	font-37	7.9701	color-0	font-60	5.9776	color-0	formula
paragraph	Θ′i ← Θi ∪ θ′i,∀i ∈I	5	375.24	545.76056	448.41727	554.6815	font-62	8.0	color-0	font-37	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	end	5	329.16	527.8201	341.56146	532.4149	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	section-heading
paragraph	Algorith  3: findPolicies	5	317.28	505.85767	431.6626	514.92365	font-21	10.0	color-0	font-235	9.9626	color-0	font-21	9.9626	color-0	unknown
paragraph	Input: I, Θ, A, p(Θ), u	5	329.16	493.5475	406.99124	501.51758	font-26	8.0	color-0	font-235	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	Output: σi, ∀i ∈I	5	329.16003	482.96176	392.01767	490.03125	font-235	8.0	color-0	font-235	7.9701	color-0	font-26	7.9701	color-0	unknown
paragraph	begin	5	329.15988	473.6997	347.7621	478.29446	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	for j ← 0 to maxNumRestarts do	5	344.52	463.3141	468.20084	470.38358	font-26	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	πi ← random, ∀i ∈I	5	359.88	454.92023	436.65994	461.71124	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	while !converged(π) do	5	359.87997	445.42752	442.61438	453.3976	font-26	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	formula
paragraph	for i ∈ I do	5	375.24	438.1499	415.19492	444.10358	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	formula
paragraph	πi ← argmax[∑θ∈Θ p(θ) ∗	5	390.6	425.76797	491.67264	436.74	font-26	8.0	color-0	font-26	7.9701	color-0	font-62	7.9701	color-0	formula
paragraph	u([πi(θi), π−i(θ−i)], θ)]	5	390.59982	416.86746	475.75055	424.83774	font-37	8.0	color-0	font-26	7.9701	color-0	font-37	7.9701	color-0	formula
paragraph	if bestSolution then	5	359.88	402.3	423.65753	406.89474	font-238	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	unknown
paragraph	σi ← πi, ∀i ∈I	5	375.24	392.00177	430.1769	399.07126	font-26	8.0	color-0	font-26	7.9701	color-0	font-26	7.9701	color-0	formula
paragraph	end	5	329.16	374.1	341.56146	378.69476	font-235	8.0	color-0	font-235	7.9701	color-0	font-235	7.9701	color-0	section-heading
paragraph	the solution to that Bayesian game.	5	317.28	354.76816	457.38705	363.74445	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Additional variables used in these algorithms are: r is the reward to the team, rs is a random seed, πi is a generic strategy, φ is a generic type, and superscripts refer to the timestep under consideration. The function beliefState(θ) returns the belief state over S given by the history of observations and actions made by all agents as defined by θ. In Algorithm 2 we specify a QMDP heuristic for calculating utility, but any other heuristic could be substituted.	5	317.2801	260.80786	553.83734	351.7445	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	For alternating-maximization, shown in its dynamic programming form in Algorithm 3, random restarts are used to move the solution out of local maxima. In order to ensure that each agent finds the same set of policies, we synchronize the random number generation of each agent.	5	317.2801	202.00758	553.7791	257.7842	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	5. Experimental Results	5	317.28	178.98	444.5742	185.87216	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	5.1. Lady and The Tiger	5	317.28	159.9	434.59647	166.18909	font-235	10.9	color-0	font-235	10.9091	color-0	font-235	10.9091	color-0	section-heading
paragraph	The Lady and The Tiger problem is a multi-agent version of the classic tiger problem [3] created by Nair et al. [8]. In this two state, finite horizon problem, two agents are faced with the problem of opening one of two doors, one of which has a tiger behind it and the other a treasure. Whenever a door is opened, the state is reset and the game continues until the fixed time has passed. The crux of the Lady and the Tiger is that neither agent can see the actions or observations made by their teammate, nor can they observe the reward signal and thereby deduce them. It is, however, necessary for an agent to reason about this information. See Nair et al. [8] for S, A, Z, T, R, and O. While small, this problem allows us to compare the policies achieved by building the full extensive form game version of the POSG to those from our Bayesian game approximation. It also shows how even a very small POSG quickly becomes intractable. Table 1 compares performance and computational time for this problem with various time horizons. The full POSG version of the problem was solved using the dynamic programming version of alternating-maximization with 20 random restarts (increasing the number of restarts did not result in higher valued policies). For the Bayesian game approximation, each agent’s type at time t consists of its entire observation and action history up to that point. Types with probability less than 0.000005 were pruned. If an agent’s true history is pruned, then it is assigned the closest matched type in Θti for action selection (using Hamming distance). The heuristic used for the utility of actions was based upon the value of policies found for shorter time horizons using the Bayesian game approximation. Our algorithm was able to find comparable policies to the full POSG in a much shorter time. Finally, to reinforce the importance of using game theory, results are included for a selfi h policy in which agents solve parallel POMDP versions of the problem using Cassandra’s publicly available pomdp-solve.2 In these POMDPs, each agent assumes that its teammate never receives observations that would lead it to independently open a door.	5	58.557976	78.767975	553.6853	553.82446	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Time Full POSG Bayesian Game Approx. selfish Horizon Time(ms) Exp. Reward Time(ms) Avg. Reward Avg. Reward 3 50 5.19 1 5.18 ± 0.15 5.14 ± 0.15 4 1000 4.80 5 4.77 ± 0.07 -28.74 ± 0.46 5 25000 7.02 20 7.10 ± 0.12 -6.59 ± 0.34 6 900000 10.38 50 10.28 ± 0.21 -42.59 ± 0.56 7 — — 200 10.00 ± 0.17 -19.86 ± 0.46 8 — — 700 12.25 ± 0.19 -56.93 ± 0.65 9 — — 3500 11.86 ± 0.14 -34.71 ± 0.56 10 — — 9000 15.07 ± 0.23 -70.85 ± 0.73	6	59.64	620.5247	300.65698	709.6549	font-237	8.0	color-0	font-235	7.9701	color-0	font-237	7.9701	color-0	table
paragraph	Table 1. Computational and performance results for Lady and the Tiger, |S| = 2. Rewards are averaged over 10000 trials with 95% confidence intervals shown.	6	70.439926	568.12036	282.81735	610.7624	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	table-caption
paragraph	5.2. Robotic Team Tag	6	58.56	144.18	167.41096	150.46909	font-235	10.9	color-0	font-235	10.9091	color-0	font-235	10.9091	color-0	section-heading
paragraph	This example is a two-robot version of Pineau et al.’s Tag problem [10]. In Team Tag, two robots are trying to herd and	6	58.560593	109.24831	294.74374	129.98445	font-237	10.0	color-0	font-237	9.9626	color-0	font-238	9.9626	color-0	body-text
paragraph	2 http://www.cs.brown.edu/research/ai/pomdp/code/index.html	6	58.56	82.810486	266.85855	89.98358	font-237	8.0	color-0	font-237	7.9701	color-0	font-237	7.9701	color-0	formula
paragraph	Permission to make digital or hard copies of all or part of  	6	18.408	72.0417	158.8314	75.9837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	this work for personal or classroom use is granted without fee  	6	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	6	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	6	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	6	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	6	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	6	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	6	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	6	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	26 27 28	6	437.7756	701.7446	475.0844	704.68024	font-216	4.9	color-0	font-216	4.909	color-0	font-216	4.909	color-0	figure
paragraph	23 24 25	6	437.7756	686.0407	475.0844	688.9665	font-216	4.9	color-0	font-216	4.909	color-0	font-216	4.909	color-0	figure
paragraph	20 21 22	6	437.7756	670.3317	475.0844	673.2575	font-216	4.9	color-0	font-216	4.909	color-0	font-216	4.909	color-0	figure
paragraph	10 11 12 13 14 15 16 17 18 19	6	359.2309	654.61774	506.50232	657.55334	font-216	4.9	color-0	font-216	4.909	color-0	font-216	4.909	color-0	figure
paragraph	0123456789	6	360.7036	638.9089	505.0296	641.8445	font-216	4.9	color-0	font-216	4.909	color-0	font-216	4.909	color-0	figure
paragraph	Figure 4. The discrete environment used for	6	329.16	616.2581	541.59595	625.6628	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	figure-caption
paragraph	Team Tag. A star indicates the goal cell.	6	329.16	604.61816	516.96716	614.0029	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	unknown
paragraph	then capture an opponent that moves with a known stochastic policy in the discrete environment depicted in Figure 4. Once the opponent is forced into the goal cell, the two robots must coordinate on a tag action in order to receive a reward and end the problem.	6	317.28	535.97797	553.6089	591.74457	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	The state space for this problem is the cross-product of the two robot positions roboti = {so, ..., s28} and the opponent position opponent = {so, ..., s28, stagged}. All three agents start in independently selected random positions and the game is over when opponent = stagged. Each robot has five actions, Ai = {North, South, East, West, Tag}. A shared reward of -1 per robot is imposed for each motion action, while the Tag action results in a +10 reward if roboto = robot1 = opponent = s28 and if both robots select Tag. Otherwise, the Tag action results in a reward of -10. The positions of the two robots are fully observable and motion actions have deterministic effects. The position of the opponent is completely unobservable unless the opponent is in the same cell as a robot and robots cannot share observations. The opponent selects actions with full knowledge of the positions of the robots and has a stochastic policy that is biased toward maximizing its distance to the closest robot.	6	317.27972	327.0704	554.0165	533.18445	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	A fully observable policy was calculated for a team that could always observe the opponent using dynamic programming for an infinite horizon version of the problem with a discount factor of 0.95. The Q-values generated for this policy were then used as a QMDP heuristic for calculating the utility of the Bayesian game approximation. Each robot’s type is its observation history with types having probability less than 0.00025 pruned.	6	317.28015	231.16814	553.91974	322.10403	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Two common heuristics for partially observable problems were also implemented. In these two heuristics, each robot maintains an individual belief state based only upon its own observations (as there is no communication) and uses that belief state to make action selections. In the Most Likely State (MLS) heuristic, the robot selects the most likely opponent state and then applies its half of the best joint action for that state as given by the fully observable policy. In the QMDP heuristic, the robot implements its half of the joint action that maximizes its expected reward given its current belief state. MLS performs better than QMDP as it does not exhibit an oscillation in action selection. As can be seen in Table 2, the Bayesian game approximation performs better than both MLS and QMDP.	6	58.56	78.76799	554.05066	513.60364	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Algorithm Avg. Discounted Value Avg. Timesteps %Success With Full Observability of Teammate’s Position Fully Observable -8.08 ± 0.11 14.39 ± 0.11 100% Most Likely State -23.44 ± 0.23 49.43 ± 0.65 78.10% QMDP 29.83- ± 0.23 63.82 ± 0.7 56.61% Bayesian Approx. -21.23 ± 0.76 43.84 ± 1.97 83.30% Without Fully Observability of Teammate’s Position Most Likely State -29.10 ± 0.25 67.31 ± 0.73 46.12% QMDP 48.35- ± 1.18 89.02 ± 0.52 16.12% Bayesian Approx. -22.21 ± 0.80 43.53 ± 1.00 83.20%	7	61.44	617.6425	292.52405	710.6148	font-237	8.0	color-0	font-237	7.9701	color-0	font-235	7.9701	color-0	table
paragraph	Table 2. Results for Team Tag, |S| = 25230. Results are averaged over 1000 trials for the Bayesian approximation and 10000 trials for the others, with 95% confidence intervals shown. Success rate is the percentage of trials in which the opponent was captured within 100 timesteps.	7	70.44	528.5178	283.0908	608.37195	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	table-caption
paragraph	The Team Tag problem was then modified by removing full observability of both robots’ positions. While each robot maintains full observability of its own position, it can only see its teammate if they are located in the same cell. The MLS and QMDP heuristics maintain an estimate of the teammate’s position by assuming that the teammate completes the other half of what the robot determines to be the best joint action at each timestep (the only information to which it has access). This estimate can only be corrected if the teammate is observed. In contrast, the Bayesian game approximation maintains a type space for each robot that includes its position as well as its observation at each timestep. The probability of such a type can be updated both by using the policies computed by the algorithm and through a mutual observation (or lack thereof). This modification really shows the strength of reasoning about the actions and experiences of others. With an opponent that tries to maximize its distance from the team, a good strategy is for the team to go down different branches of the main corridor and then herd the opponent toward the goal state. With MLS and QMDP, the robots frequently go down the same branch because they both incorrectly make the assumption that their teammate will go down the other side. With the Bayesian game approximation, however, the robots were able to better track the true position of their teammate. The performance of these approaches under this modification are shown in Table 2, and it can be seen that while the Bayesian approximation performs comparably to how it did with full observability of both robot positions, the other two algorithms do not.	7	58.56024	151.2481	295.0616	500.78473	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	5.3. Robotic Tag 2	7	58.56	125.82	147.16368	132.1091	font-235	10.9	color-0	font-235	10.9091	color-0	font-235	10.9091	color-0	unknown
paragraph	In order to test the Bayesian game approximation as a real-time controller for robots, we implemented a modified version of Team Tag in a portion of Stanford University’s Permission to make digital or hard copies of all or part of  	7	18.408	72.0417	294.92813	111.144455	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	7	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	7	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	7	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	7	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	7	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	7	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	7	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	7	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	Figure 5. The robot team running in simulation with no opponent present. Robot paths and current goals (light coloured circles) are	7	329.15997	585.41846	541.61487	618.3229	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	figure-caption
paragraph	shown.	7	329.15997	575.80054	363.4896	583.09314	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	unknown
paragraph	Algorithm Avg. Discounted Value Avg. Timesteps %Success Fully Observable 2.70 ± 0.09 5.38 ± 0.06 100% Most Likely State -7.04 ± 0.21 16.47 ± 0.28 99.96% QMDP 9.54- ± 0.27 25.29 ± 0.54 95.37% Bayesian Approx. -6.01 ± 0.21 15.20 ± 0.27 99.94%	7	320.16	505.08252	551.36395	550.1748	font-237	8.0	color-0	font-237	7.9701	color-0	font-235	7.9701	color-0	table
paragraph	Table 3. Results for Tag 2, |S| = 18252. Results are averaged over 10000 trials with 95% con-	7	329.16	474.5378	541.65265	495.81195	font-218	10.0	color-0	font-218	9.9626	color-0	font-218	9.9626	color-0	table-caption
paragraph	fidence intervals shown.	7	329.161	464.80038	444.8347	472.18265	font-218	10.0	color-0	font-218	1.0	color-0	font-218	9.9626	color-0	unknown
paragraph	Gates Hall as shown in Figure 5. The parameters of this problem are similar to those of Team Tag with full teammate observability except that: the opponent moves with Brownian motion; the opponent can be captured in any state; and only one member of the team is required to tag the opponent. As with Team Tag, the utility for the problem was calculated by solving the underlying fully-observable game in a grid-world. MLS, QMDP and the Bayesian game approximation were then applied to this grid-world to generate the performance statistics shown in Table 3. Once again, the Bayesian approximation outperforms both MLS and QMDP.	7	317.279	310.7274	553.75555	448.3943	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	The grid-world was also mapped to the real Gates Hall and two Pioneer-class robots using the Carmen software package for low-level control.  In this mapping, observations and actions remain discrete and there is an additional layer that converts from the continuous world to the grid-world and back again. Localized positions of the robots are used to calculate their specific grid-cell positions, with each cell being roughly 1.0m×3.5m. Robots navigate by convert-ing the actions selected by the Bayesian game approximation into goal locations. For runs in the real environment, no opponent was used and the robots were always given a null observation. (As a result, the robots continued to hunt for an opponent until stopped by a human operator.) Figure 5 shows a set of paths taken by robots in a simulation of this environment as they attempt to capture and tag the (non-existent) opponent. Runs on the physical robots generated similar paths for the same initial conditions.	7	317.27933	110.09744	553.9271	306.5041	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	3 http://www.cs.cmu.edu/̃ carmen	7	317.28	82.810486	435.03305	89.98358	font-237	8.0	color-0	font-237	7.9701	color-0	font-237	7.9701	color-0	unknown
paragraph	6. Discussion	8	58.56	709.98	129.61693	716.87213	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	We have presented an algorithm that deals with the intractability of POSGs by transforming them into a series of smaller Bayesian games. Each of these games can then be solved to find one-step policies that, together, approximate the globally optimal solution of the original POSG. The Bayesian games are kept efficient through pruning of low-probability histories and heuristics to calculate the utility of actions. This results in policies for the POSG that are locally optimal with respect to the heuristic used. There are several frameworks that have been proposed to generalize POMDPs to distributed, multi-agent systems. DEC-POMDP [1], a model of decentralized partially observable Markov decision processes, and MTDP [12], a Markov team decision problem, are both examples of these frameworks. They formalize the requirements of an optimal policy; however, as shown by Bernstein et al., solving decentralized POMDPs is NEXP-complete [1]. I-POMDP [5] also generalizes POMDPs but uses Bayesian games and decision theory to augment the state space to include models of other agents’ behaviour. There is, however, little to suggest that these I-POMDPs can then be solved optimally. These results reinforce our belief that locally optimal policies that are computationally efficient to generate are essential to the success of applying the POSG framework to the real world.	8	58.5596	403.00085	295.2799	693.1445	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Algorithms that attempt to find locally optimal solutions include POIPSG [9] which is a model of partially observable identical payoff stochastic games in which gradient descent search is used to find locally optimal policies from a limited set of policies. Rather than limit policy space, Xuan et. al. [16] deal with decentralized POMDPs by restricting the type of partial observability in their system. Each agent receives only local information about its position and so agents only ever have complementary observations. The issue in this system then becomes the determination of when global information is necessary to make progress toward the goal, rather than how to resolve conflicts in beliefs or to augment one’s own belief about the global state. The dynamic programming version of the alternating-maximization algorithm we use for finding solutions for extensive form games is very similar to the work done by Nair et. al. but with a difference in policy representation [8]. This alternating-maximization algorithm, however, is just one subroutine in our larger Bayesian game approximation; our approximation allows us to find solutions to much larger problems than any exact algorithm could handle. The Bayesian game approximation for finding solutions to POSGs has two main areas for future work. First, because the performance of the approximation is limited by the quality of the heuristic used for the utility function, we plan to investigate heuristics, such as policy values of centralized POMDPs, that would allow our algorithm to consider the Permission to make digital or hard copies of all or part of  	8	18.408	72.0417	295.20316	396.2651	font-237	10.0	color-0	font-290	6.0	color-0	font-237	9.9626	color-0	body-text
paragraph	this work for personal or classroom use is granted without fee  	8	18.407997	65.1417	169.64766	69.0837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	provided that copies are not made or distributed for profit or  	8	18.407997	58.2417	166.02092	62.1837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	commercial advantage and that copies bear this notice and the  	8	18.407997	51.341698	170.22227	55.2837	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	full citation on the first page. To copy otherwise, to republish,  	8	18.407997	44.441696	170.24304	48.383698	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	to post on servers or to redistribute to lists, requires prior  	8	18.407997	37.541695	157.83823	41.483696	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	unknown
paragraph	specific permission and/or a fee.  	8	18.407997	30.641695	99.15059	34.583694	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           AAMAS'04, July 19-23, 2004, New York, New York, USA.  	8	18.407997	23.741695	183.0073	27.683695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	formula
paragraph	           Copyright 2004 ACM 1-58113-864-4/04/0007...$5.00 	8	18.407997	16.841696	166.26	20.783695	font-290	6.0	color-0	font-290	6.0	color-0	font-290	6.0	color-0	page-footer
paragraph	effects of uncertainty beyond the immediate action selection. The second area is in improving the efficiency of the type profile space coverage and representation.	8	317.27835	684.4064	553.5513	716.78284	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	Acknowledgments	8	317.28	663.54	410.80554	670.4321	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	section-heading
paragraph	Rosemary Emery-Montemerlo is supported in part by a Natural Sciences and Engineering Research Council of Canada postgraduate scholarship, and this research has been sponsored by DARPA’s MICA program.	8	317.2796	607.8478	553.9675	651.86456	font-237	10.0	color-0	font-237	9.9626	color-0	font-237	9.9626	color-0	body-text
paragraph	References	8	317.28	588.42	372.79874	595.31213	font-235	12.0	color-0	font-235	11.9552	color-0	font-235	11.9552	color-0	reference-heading
paragraph	[1] D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of decentralized control of Markov decision processes. In UAI, 2000.	8	321.72	544.5558	553.4751	573.424	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[2] C. Boutilier. Planning, learning and coordination in multiagent decision processes. In Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, 1996.	8	321.72092	502.66296	553.90857	541.14404	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[3] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Acting optimally in partially observable stochastic domains. In AAAI, 1994.	8	321.72092	469.31607	553.5535	498.18427	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[4] D. Fudenberg and J. Tirole. Game Theory. MIT Press, Cambridge, MA, 1991.	8	321.72095	447.0256	553.57434	465.90436	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[5] P. J. Gmytrasiewicz and P. Doshi. A framework for sequential planning in multi-agent settings. In Proceedings of the 8th International Symposium on Artificial Intelligence and Mathematics, Ft. Lauderdale, Florida, January 2004. [6] D. Koller, N. Megiddo, and B. von Stengel. Efficient computation of equilibria for extensive two-person games. Games and Economic Behavior, 14(2):247–259, June 1996. [7] M. L. Littman, A. R. Cassandra, and L. P. Kaelbling. Learning policies for partially observable environments: Scaling up. In ICML, 1995.	8	321.72092	339.39484	553.97754	444.34915	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[8] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella. Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In IJCAI, 2003. [9] L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling.	8	321.7218	296.30594	553.4809	336.66458	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	Learning to cooperate via policy search. In UAI, 2000.	8	337.2023	285.62607	532.94727	293.7048	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[10] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration:	8	317.28253	275.38196	553.5535	282.9048	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	An anytime algorithm for POMDPs. In UAI, 2003. [11] J. Pineau, M. Montemerlo, M. Pollack, N. Roy, and S. Thrun.	8	317.28162	253.226	553.4064	272.10477	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	Towards robotic assistants in nursing homes: Challenges and results. Robotics and Autonomous Systems, 42(3-4):271–281, March 2003.	8	337.2014	221.7566	553.6985	250.62485	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	[12] D. Pynadath and M. Tambe. The communicative multiagent	8	317.28162	210.26616	553.4443	218.3449	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	team decision problem: Analyzing teamwork theories and models. Journal of Artificial Intelligence Research, 2002. [13] S. Russell and P. Norvig. Section 6.5: Games that include	8	317.2816	177.86606	553.5238	207.54486	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	an element of chance. In Artifi ial Intelli : A Modern Approach. Prentice Hall, 2nd edition, 2002.	8	337.20135	157.07655	553.51117	175.26491	font-237	9.0	color-0	font-237	8.9664	color-0	font-238	8.9664	color-0	reference
paragraph	[14] J. Shi and M. L. Littman. Abstraction methods for game theoretic	8	317.28162	145.5861	553.3478	153.66484	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	poker. Computers and Games, pages 333–345, 2000. [15] B. von Stengel. Efficient computation of behavior strategies.	8	317.28162	124.1062	553.5655	143.02979	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	Games and Economic Behavior, 14(2):220–246, June 1996. [16] P. Xuan, V. Lesser, and S. Zilberstein. Communication decisions	8	317.28073	103.062065	553.4081	121.39387	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
paragraph	in multi-agent cooperation: Model and experiments. In Agents-01, 2001.	8	337.2005	81.71666	553.5168	99.905	font-237	9.0	color-0	font-237	8.9664	color-0	font-237	8.9664	color-0	reference
font	font-15	cmex10	0	0	0
font	font-21	cmmi10	0	1	0
font	font-216	courier	0	0	0
font	font-218	helvetica-bold	1	0	0
font	font-222	helvetica	0	0	0
font	font-23	cmmi5	0	1	0
font	font-234	symbol	0	0	0
font	font-235	times-bold	1	0	0
font	font-236	times-bolditalic	1	1	0
font	font-237	times-roman	0	0	0
font	font-238	times-italic	0	1	0
font	font-24	cmmi6	0	1	0
font	font-25	cmmi7	0	1	0
font	font-26	cmmi8	0	1	0
font	font-290	timesnewroman	0	0	0
font	font-31	cmr10	0	0	0
font	font-35	cmr6	0	0	0
font	font-36	cmr7	0	0	0
font	font-37	cmr8	0	0	0
font	font-58	cmsy10	0	1	0
font	font-59	cmsy5	0	1	0
font	font-60	cmsy6	0	1	0
font	font-61	cmsy7	0	1	0
font	font-62	cmsy8	0	1	0
color	color-0	0.0	0.0	0.0
color	color-1	0.0	0.0	0.0
color	color-3	0.8269932	0.8269932	0.8269932